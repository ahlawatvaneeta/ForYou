# -*- coding: utf-8 -*-
"""misahub_medical.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iskU7TSOQ_AXj2nvaBSeJwyobGUgihIM
"""

import zipfile
zip_ref=zipfile.ZipFile('/content/drive/MyDrive/ColabNotebooks/Dataset.zip')
zip_ref.extractall('/content/drive/MyDrive/ColabNotebooks')
zip_ref.close()

import pandas as pd  # Used to load CSV files containing the data.
from torchvision import transforms  # Provides image transformations.
from torch.utils.data import DataLoader, Dataset  # Contains DataLoader and Dataset classes.
from PIL import Image  # Used to open and manipulate image files.
import os  # To handle file paths.
import torch
import numpy as np
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, multilabel_confusion_matrix, accuracy_score
import torch.nn as nn
import torch.optim as optim


# Check if T4 GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

# Define the custom dataset class
class DiseaseDataset(Dataset):
    def __init__(self, dataframe, img_dir, transform=None):
        self.dataframe = dataframe
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.dataframe.iloc[idx, 0])
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image not found: {img_path}")
        image = Image.open(img_path).convert('RGB')
        labels = self.dataframe.iloc[idx, 2:].values.astype(float)
        if self.transform:
            image = self.transform(image)
        return image, labels


# Load the training and validation datasets
train_file_path = '/content/drive/MyDrive/ColabNotebooks/Dataset/training/training_data_csv.csv'
val_file_path = '/content/drive/MyDrive/ColabNotebooks/Dataset/validation/validation_data_csv.csv'

train_data = pd.read_csv(train_file_path)  # Load the training dataset CSV
val_data = pd.read_csv(val_file_path)  # Load the validation dataset CSV

# Image transformations
# Data Augmentation
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally
    transforms.RandomRotation(10),  # Randomly rotate the image by 10 degrees
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Adjust brightness, contrast, saturation
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])


# Create datasets
train_dataset = DiseaseDataset(train_data, img_dir='/content/drive/MyDrive/ColabNotebooks/Dataset/', transform=transform)
val_dataset = DiseaseDataset(val_data, img_dir='/content/drive/MyDrive/ColabNotebooks/Dataset/', transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)

# Define the path where the loaders will be saved
train_loader_path = '/content/drive/MyDrive/ColabNotebooks/train_loader.pth'
val_loader_path = '/content/drive/MyDrive/ColabNotebooks/val_loader.pth'

# Save the train and validation loaders
torch.save(train_loader, train_loader_path)
torch.save(val_loader, val_loader_path)

print("DataLoaders have been saved successfully!")

class CNNModel(nn.Module):
    """
    Define the CNN architecture.
    """
    def __init__(self, num_labels):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.dropout = nn.Dropout(0.5)

        # Determine the output size of the convolutional layers dynamically
        self._initialize_fc()

        self.fc1 = nn.Linear(self.flattened_size, 128)
        self.fc2 = nn.Linear(128, num_labels)
        self.sigmoid = nn.Sigmoid()

    def _initialize_fc(self):
        dummy_input = torch.zeros(1, 3, 256, 256)
        x = self.pool(torch.relu(self.conv1(dummy_input)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        self.flattened_size = x.numel()

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return self.sigmoid(x)

# Instantiate the model
num_labels = 10  # Adjust this to match the number of output labels in your dataset
model = CNNModel(num_labels).to(device)  # Move the model to GPU if available
cnn_model = '/content/drive/MyDrive/ColabNotebooks/cnn_model.pth'

# Save the model architecture for later use
torch.save(model.state_dict(), cnn_model)

# Function to evaluate the model
def evaluate_model(y_true, y_pred_proba, y_pred):
    roc_auc = []
    for i in range(y_true.shape[1]):
        try:
            auc = roc_auc_score(y_true[:, i], y_pred_proba[:, i])
        except ValueError:
            auc = np.nan
        roc_auc.append(auc)
    avg_roc_auc = np.nanmean(roc_auc)

    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)
    avg_precision = np.mean(precision)
    avg_recall = np.mean(recall)
    avg_f1 = np.mean(f1)

    accuracies = [accuracy_score(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])]
    avg_accuracy = np.mean(accuracies)

    metrics = {
        "avg_roc_auc": avg_roc_auc,
        "avg_precision": avg_precision,
        "avg_recall": avg_recall,
        "avg_f1_score": avg_f1,
        "avg_accuracy": avg_accuracy
    }

    return metrics

# Define loss and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Load pre-saved train_loader and val_loader
train_loader_path = '/content/drive/MyDrive/ColabNotebooks/train_loader.pth'
val_loader_path = '/content/drive/MyDrive/ColabNotebooks/val_loader.pth'

train_loader = torch.load(train_loader_path)
val_loader = torch.load(val_loader_path)

# address to save model
best_cnn_weights='/content/drive/MyDrive/ColabNotebooks/best_cnn_weights.pth'

# Initialize lists to store metrics for each epoch
training_losses = []
validation_losses = []
training_auc_scores = []
validation_auc_scores = []
training_f1_scores = []
validation_f1_scores = []
training_accuracies = []
validation_accuracies = []

# Training loop
epochs = 40
best_val_loss = float('inf')
best_val_auc = 0.0

for epoch in range(epochs):
    model.train()  # Set model to training mode
    running_loss = 0.0
    all_train_labels = []
    all_train_outputs = []
    all_train_outputs_proba = []

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        all_train_labels.append(labels.cpu().numpy())
        all_train_outputs_proba.append(torch.sigmoid(outputs).detach().cpu().numpy())
        all_train_outputs.append((torch.sigmoid(outputs).detach().cpu().numpy() > 0.5).astype(int))

    avg_train_loss = running_loss / len(train_loader)
    all_train_labels = np.vstack(all_train_labels)
    all_train_outputs_proba = np.vstack(all_train_outputs_proba)
    all_train_outputs = np.vstack(all_train_outputs)

    # Evaluate training metrics
    train_metrics = evaluate_model(all_train_labels, all_train_outputs_proba, all_train_outputs)

    # Store training metrics
    training_losses.append(avg_train_loss)
    training_auc_scores.append(train_metrics['avg_roc_auc'])
    training_f1_scores.append(train_metrics['avg_f1_score'])
    training_accuracies.append(train_metrics['avg_accuracy'])

    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0.0
    all_val_labels = []
    all_val_outputs = []
    all_val_outputs_proba = []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            all_val_labels.append(labels.cpu().numpy())
            all_val_outputs_proba.append(torch.sigmoid(outputs).detach().cpu().numpy())
            all_val_outputs.append((torch.sigmoid(outputs).detach().cpu().numpy() > 0.5).astype(int))

    avg_val_loss = val_loss / len(val_loader)
    all_val_labels = np.vstack(all_val_labels)
    all_val_outputs_proba = np.vstack(all_val_outputs_proba)
    all_val_outputs = np.vstack(all_val_outputs)

    # Evaluate validation metrics
    val_metrics = evaluate_model(all_val_labels, all_val_outputs_proba, all_val_outputs)

    # Store validation metrics
    validation_losses.append(avg_val_loss)
    validation_auc_scores.append(val_metrics['avg_roc_auc'])
    validation_f1_scores.append(val_metrics['avg_f1_score'])
    validation_accuracies.append(val_metrics['avg_accuracy'])

    # Print metrics for the current epoch
    print(f'Epoch [{epoch + 1}/{epochs}], '
          f'Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')
    print(f'Training Metrics: AUC-ROC: {train_metrics["avg_roc_auc"]:.4f}, '
          f'F1-score: {train_metrics["avg_f1_score"]:.4f}, '
          f'Accuracy: {train_metrics["avg_accuracy"]:.4f}')
    print(f'Validation Metrics: AUC-ROC: {val_metrics["avg_roc_auc"]:.4f}, '
          f'F1-score: {val_metrics["avg_f1_score"]:.4f}, '
          f'Accuracy: {val_metrics["avg_accuracy"]:.4f}')

    # Save the best model
    if avg_val_loss < best_val_loss or val_metrics['avg_roc_auc'] > best_val_auc:
        best_val_loss = avg_val_loss
        best_val_auc = val_metrics['avg_roc_auc']
        torch.save(model.state_dict(), best_cnn_weights)
        print(f'Saved model with validation loss: {best_val_loss:.4f} and AUC-ROC: {best_val_auc:.4f}')

# Final printout of training completion
print("\nTraining completed! Metrics for all epochs:")
for epoch in range(epochs):
    print(f"Epoch [{epoch + 1}/{epochs}]: "
          f"Training Loss: {training_losses[epoch]:.4f}, Validation Loss: {validation_losses[epoch]:.4f}, "
          f"Training AUC-ROC: {training_auc_scores[epoch]:.4f}, Validation AUC-ROC: {validation_auc_scores[epoch]:.4f}, "
          f"Training F1-score: {training_f1_scores[epoch]:.4f}, Validation F1-score: {validation_f1_scores[epoch]:.4f}, "
          f"Training Accuracy: {training_accuracies[epoch]:.4f}, Validation Accuracy: {validation_accuracies[epoch]:.4f}")

Epoch [1/40], Training Loss: 0.6792, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0874, Accuracy: 0.9520
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Saved model with validation loss: 0.6791 and AUC-ROC: 0.5000
Epoch [2/40], Training Loss: 0.6791, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0867, Accuracy: 0.9521
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [3/40], Training Loss: 0.6834, Validation Loss: 0.6931
Training Metrics: AUC-ROC: 0.5003, F1-score: 0.0810, Accuracy: 0.9386
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0000, Accuracy: 0.9000
Epoch [4/40], Training Loss: 0.6868, Validation Loss: 0.6931
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0627, Accuracy: 0.9248
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0000, Accuracy: 0.9000
Epoch [5/40], Training Loss: 0.6807, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0859, Accuracy: 0.9479
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [6/40], Training Loss: 0.6795, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0866, Accuracy: 0.9508
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [7/40], Training Loss: 0.6795, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4998, F1-score: 0.0857, Accuracy: 0.9508
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [8/40], Training Loss: 0.6793, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5001, F1-score: 0.0868, Accuracy: 0.9515
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [9/40], Training Loss: 0.6792, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0871, Accuracy: 0.9520
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [10/40], Training Loss: 0.6792, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0873, Accuracy: 0.9519
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [11/40], Training Loss: 0.6793, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0870, Accuracy: 0.9519
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [12/40], Training Loss: 0.6792, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0870, Accuracy: 0.9519
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [13/40], Training Loss: 0.6792, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0867, Accuracy: 0.9520
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [14/40], Training Loss: 0.6792, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0868, Accuracy: 0.9520
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [15/40], Training Loss: 0.6792, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5002, F1-score: 0.0878, Accuracy: 0.9520
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [16/40], Training Loss: 0.6793, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5002, F1-score: 0.0882, Accuracy: 0.9518
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [17/40], Training Loss: 0.6794, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0879, Accuracy: 0.9516
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [18/40], Training Loss: 0.6793, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4998, F1-score: 0.0867, Accuracy: 0.9519
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [19/40], Training Loss: 0.6791, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0870, Accuracy: 0.9522
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [20/40], Training Loss: 0.6791, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5001, F1-score: 0.0872, Accuracy: 0.9521
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [21/40], Training Loss: 0.6791, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0868, Accuracy: 0.9522
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [22/40], Training Loss: 0.6791, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.4999, F1-score: 0.0866, Accuracy: 0.9522
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [23/40], Training Loss: 0.6790, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0867, Accuracy: 0.9523
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [24/40], Training Loss: 0.6790, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0866, Accuracy: 0.9523
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [25/40], Training Loss: 0.6790, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0868, Accuracy: 0.9523
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [26/40], Training Loss: 0.6790, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5001, F1-score: 0.0869, Accuracy: 0.9523
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523
Epoch [27/40], Training Loss: 0.6790, Validation Loss: 0.6791
Training Metrics: AUC-ROC: 0.5000, F1-score: 0.0866, Accuracy: 0.9524
Validation Metrics: AUC-ROC: 0.5000, F1-score: 0.0865, Accuracy: 0.9523

!pip install fastapi

# Import libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score
import numpy as np
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os
import pandas as pd

# Address to save model
best_cnn_weights = '/content/drive/MyDrive/ColabNotebooks/best_cnn_weights1.pth'

# Data Augmentation and Transformations
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Custom Dataset class remains the same
class DiseaseDataset(Dataset):
    def __init__(self, dataframe, img_dir, transform=None):
        self.dataframe = dataframe
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.dataframe.iloc[idx, 0])
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image not found: {img_path}")
        image = Image.open(img_path).convert('RGB')
        labels = self.dataframe.iloc[idx, 2:].values.astype(float)
        if self.transform:
            image = self.transform(image)
        return image, labels

# Loaders
train_loader = torch.load('/content/drive/MyDrive/ColabNotebooks/train_loader.pth')
val_loader = torch.load('/content/drive/MyDrive/ColabNotebooks/val_loader.pth')

# Define the CNN architecture
class CNNModel(nn.Module):
    def __init__(self, num_labels):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.dropout = nn.Dropout(0.5)

        # Dynamically calculate the flattened size for fully connected layers
        self._initialize_fc()

        self.fc1 = nn.Linear(self.flattened_size, 128)
        self.fc2 = nn.Linear(128, num_labels)
        self.sigmoid = nn.Sigmoid()

    def _initialize_fc(self):
        # Run a dummy forward pass to determine the size of the flattened tensor
        dummy_input = torch.zeros(1, 3, 256, 256)  # Assuming input size of (3, 256, 256)
        x = self.pool(torch.relu(self.conv1(dummy_input)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        self.flattened_size = x.numel()

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return self.sigmoid(x)

# Instantiate the model
num_labels = 10  # Adjust this to match the number of output labels in your dataset
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNModel(num_labels).to(device)  # Move the model to GPU if available

# Define loss and optimizer with weight decay (L2 regularization)
criterion = nn.BCEWithLogitsLoss()  # Better numerical stability for multi-label classification
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# Learning rate scheduler
scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# Save model's architecture for future use
cnn_model_path = '/content/drive/MyDrive/ColabNotebooks/cnn_model.pth'
torch.save(model.state_dict(), cnn_model_path)

# Training loop with early stopping
epochs = 40
best_val_loss = float('inf')
best_val_auc = 0.0
patience = 5  # Early stopping patience
early_stop_counter = 0

for epoch in range(epochs):
    model.train()  # Set model to training mode
    running_loss = 0.0
    all_train_labels = []
    all_train_outputs = []
    all_train_outputs_proba = []

    # Training phase
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        all_train_labels.append(labels.cpu().numpy())
        all_train_outputs_proba.append(torch.sigmoid(outputs).detach().cpu().numpy())
        all_train_outputs.append((torch.sigmoid(outputs).detach().cpu().numpy() > 0.5).astype(int))

    avg_train_loss = running_loss / len(train_loader)
    all_train_labels = np.vstack(all_train_labels)
    all_train_outputs_proba = np.vstack(all_train_outputs_proba)
    all_train_outputs = np.vstack(all_train_outputs)

    # Evaluate training metrics
    train_metrics = evaluate_model(all_train_labels, all_train_outputs_proba, all_train_outputs)

    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0.0
    all_val_labels = []
    all_val_outputs = []
    all_val_outputs_proba = []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            all_val_labels.append(labels.cpu().numpy())
            all_val_outputs_proba.append(torch.sigmoid(outputs).detach().cpu().numpy())
            all_val_outputs.append((torch.sigmoid(outputs).detach().cpu().numpy() > 0.5).astype(int))

    avg_val_loss = val_loss / len(val_loader)
    all_val_labels = np.vstack(all_val_labels)
    all_val_outputs_proba = np.vstack(all_val_outputs_proba)
    all_val_outputs = np.vstack(all_val_outputs)

    # Evaluate validation metrics
    val_metrics = evaluate_model(all_val_labels, all_val_outputs_proba, all_val_outputs)

    # Print metrics for the current epoch
    print(f'Epoch [{epoch + 1}/{epochs}], '
          f'Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')
    print(f'Training Metrics: AUC-ROC: {train_metrics["avg_roc_auc"]:.4f}, '
          f'F1-score: {train_metrics["avg_f1_score"]:.4f}, '
          f'Accuracy: {train_metrics["avg_accuracy"]:.4f}')
    print(f'Validation Metrics: AUC-ROC: {val_metrics["avg_roc_auc"]:.4f}, '
          f'F1-score: {val_metrics["avg_f1_score"]:.4f}, '
          f'Accuracy: {val_metrics["avg_accuracy"]:.4f}')

    # Save the best model
    if avg_val_loss < best_val_loss or val_metrics['avg_roc_auc'] > best_val_auc:
        best_val_loss = avg_val_loss
        best_val_auc = val_metrics['avg_roc_auc']
        torch.save(model.state_dict(), best_cnn_weights)
        print(f'Saved model with validation loss: {best_val_loss:.4f} and AUC-ROC: {best_val_auc:.4f}')
        early_stop_counter = 0  # Reset early stopping counter
    else:
        early_stop_counter += 1  # Increment counter if no improvement

    # Check early stopping condition
    if early_stop_counter >= patience:
        print(f"Early stopping triggered after {epoch + 1} epochs.")
        break

    # Step the learning rate scheduler
    scheduler.step()

# Import necessary libraries
import torch
import torch.nn as nn
from torchvision import transforms

# Define your CNN model class (ensure it matches your training definition)
class CNNModel(nn.Module):
    def __init__(self, num_labels):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.dropout = nn.Dropout(0.5)
        self._initialize_fc()

        self.fc1 = nn.Linear(self.flattened_size, 128)
        self.fc2 = nn.Linear(128, num_labels)
        self.sigmoid = nn.Sigmoid()

    def _initialize_fc(self):
        dummy_input = torch.zeros(1, 3, 256, 256)
        x = self.pool(torch.relu(self.conv1(dummy_input)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        self.flattened_size = x.numel()

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return self.sigmoid(x)

# Load your model
num_labels = 10  # Adjust based on your specific model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNModel(num_labels).to(device)

# Load the saved weights
# Load the saved weights, specifying map_location
model.load_state_dict(torch.load('/content/drive/MyDrive/ColabNotebooks/best_cnn_weights1.pth', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))
model.eval()  # Set the model to evaluation mode

model=CNNModel()
import pickle

# Define the path where you want to save the model
save_path = '/content/drive/MyDrive/ColabNotebooks/model.pkl'

# Save the model using pickle
with open(save_path, 'wb') as f:
    pickle.dump(model, f)

print(f"Model saved as {save_path}")